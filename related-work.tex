
	
	\section{Related Work}
	Our work mainly involves the relational graph convolutional networks (R-GCNs)~\cite{Schlichtkrull2017Modeling}. The R-GCN model is an extension of graph convolutional networks (GCNs)~\cite{Kipf2016Semi} to large-scale relational data. Then, we will introduce them in detail and provide a systematic review of current work on entity alignment.
	\subsection*{Graph Convolutional Networks}
	Recently, there is an increasing interest in extending neural networks to deal with arbitrarily structured graphs and there have been many encouraging works. Among them, GCNs~\cite{Duvenaud2015Convolutional,Kipf2016Semi,Kearnes2016Molecular}, a recent class of multilayer neural networks operating on graphs, have been successfully applied to semi-supervised node classification~\cite{Kipf2016Semi}, semantic role labeling~\cite{Marcheggiani2017Encoding}, neural machine translation~\cite{Bastings2017Graph} and so on. For every node in the graph, GCN encodes relevant information about its neighborhood and it is concerned with the adjacent node features. 
	\subsection*{Relational Graph Convolutional Networks}
	Since GCNs~\cite{Kipf2016Semi} generally operate on undirected and unlabeled graphs, R-GCNs~\cite{Schlichtkrull2017Modeling} are developed specifically for relational (directed and labeled) multi-graphs to deal with the highly multi-relational data characteristic of realistic knowledge graphs. And this model has been successfully exploited in two standard knowledge base completion tasks: Link prediction and entity classification~\cite{Schlichtkrull2017Modeling}. In this paper, we successfully construct the entity alignment model which utilizes R-GCNs to embed entities of multiple KGs into a unified vector space.
	
	\subsection*{Entity Alignment}
	
	As we mentioned in Section~\ref{section:intro}, the conventional entity alignment approaches are usually time-consuming and laborious since that the traditional works generally rely on external information and require costly manual feature construction. For example, in the work of Wang Xuepeng et al.~\shortcite{Wang2017}, they need to collect various network semantic labels such as category labels, attribute labels and unstructured text keywords of the entity entries to build a number of semantic similarity calculation models.
	
	To address these issues, several embedding-based methods have been proposed and achieve promising results, such as JE~\cite{hao2016joint}, MTransE~\cite{chen2016multilingual}, JAPE~\cite{sun2017cross} and ITransE~\cite{zhu2017iterative}. 
	
	
	Following the energy-based framework in TransE, JE jointly learns the embeddings of multiple KGs in a uniform vector space to align entities in KGs by adding the loss of alignment part to the global loss function. In the learning process, JE requires the seed aligned entities share the same embeddings. Although TransE can effectively capture the structure information of KGs, there are still several tough situations where TransE can not perform very well. Since TransE utilizes the relation between the head entity and the tail entity to define the distance between the head entity vector and the tail entity vector, the TransE-based approaches actually tend to require that the neighboring structures of aligned entities should be as similar as possible. Due to the incompleteness of knowledge graphs, the densities of the neighborhoods of the two entities $e_1$ and $e_2$ that we need to align may be very different or there are few similar neighbors between two entities, which leads to sparse available clues for alignment (Figure~\ref{Wuhan} gives an example), which will make a large difference between the learned vectors of $e_1$ and $e_2$ by TransE. 
	
	Since utilizing TransE to embed entities, MTransE, JAPE and ITransE have the same problems as JE. In addition, besides the pre-aligned entities, MTransE needs a set of triples to be aligned in advance and ITransE needs pre-aligned relations. JAPE needs both relations and attributes to be aligned. Actually, the seed alignments required by MTransE, ITransE and JAPE are difficult to obtain in practice.
	
	These embedding-based approaches all ignored the value information except JAPE. However, values are actually very significant parts of KGs, especially for low-quality KGs in which the entity linking is insufficient. Those low-quality KGs may contain large-scale values and the approaches that do not consider values will lose this part of the information when aligning KGs. JAPE is a joint attribute-preserving embedding model for cross-lingual entity alignment. And JAPE proposed attribute embedding to represent the attribute correlations of KGs which considers the value information, only the type of values.
	
	Instead of utilizing TransE, our approach leverages R-GCNs~\cite{Schlichtkrull2017Modeling} to better characterize entities by incorporating the neighboring relational structure information and considers the value information in multiple KGs.
