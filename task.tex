	
%	\section{Problem Formulation}
%	We first introduce the notations used in this paper. A knowledge graph is formalized as $KG = (E,V,R,A,T)$, where $E,V,R,A,T$ are the
%sets of entities, values, relations, attributes and triples respectively. KG consists of knowledge triples described as $(s, p, o)$, in
%which $s \in E$, $p \in R$ and $o \in E$ or $p \in A$ and $o \in V$.
%	
%	The task of entity alignment aims to link entities and their counterparts among multiple KGs. Suppose there are two knowledge graphs
%$KG_1 = (E_1,V_1,R_1,A_1,T_1)$ and $KG_2 = (E_2,V_2,R_2,A_2,T_2)$, and $\mathbb{L} = \{(e_i^1 , e_i^2 )|e_i^1 \in E_1, e_i^2 \in E_2,
%i\in\{1,...,n\}\}$ is a set of pre-aligned entity pairs between $KG_1$ and $KG_2$. The task of entity alignment is to automatically
%discover new entity alignments based on known alignment seeds.
	
	\section{Preliminary}
	
	\subsection*{Graph Convolutional Networks}
	\label{section:Graph Convolutional Networks}
	Here we briefly introduce Graph Convolutional Networks (GCNs)~\cite{Kipf2016Semi}. The inputs of a GCN model are a set of node features and the adjacency matrix of the undirected graph. The output is a new set of node features, containing information of neighbor nodes. GCNs can convolve the $k$th-order neighborhood information by stacking multiple convolutional layers.
	
	Formally, consider an undirected graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ and $\mathcal{E}$ are sets of nodes and edges, respectively. A GCN layer $l+1$ takes current node features $h^{(l)}=\{h_1^{(l)},h_2^{(l)},...,h_n^{(l)}\}, h_i^{(l)} \in \mathbb{R}^{F^{(l)}}$ and the adjacency matrix $A$ as input, and produces new node representations, $h^{(l+1)}=\{h_1^{(l+1)},h_2^{(l+1)},...,h_n^{(l+1)}\}, h_i^{(l+1)} \in \mathbb{R}^{F^{(l+1)}}$, as output. Here $F^{(l)}$ represents the size of node features in the $l$th layer. $n=|\mathcal{V}|$, indicates the number of nodes. The layer-wise propagation rule can be simply expressed as:
	\begin{equation}
		h_i^{(l+1)}=\sigma (\sum_{j \in \mathcal{N}_i}\hat A_{ij}h_j^{(l)}W^{(l)}+b^{(l)})
	\end{equation}
	where $\hat A=\hat D^{- \frac{1}{2}}(A+I)\hat D^{- \frac{1}{2}}$, $\hat D$ is the degree matrix of $A+I$. $\sigma(\cdot)$ is an activation function. $\mathcal{N}_i$ denotes the set of neighbor indices of node $i$ in the graph, including node $i$, and $W^{(l)}$ and $b^{(l)}$ are a weight matrix and a bias of layer $l$.
	
	\subsection*{Relational Graph Convolutional Networks}
	\label{section:Relational Graph Convolutional Networks}
	GCNs have been shown to be very effective at accumulating and encoding features from local, structured neighborhoods. Inspired by these architectures, a simple propagation model for relational (directed and labeled) multi-graphs, R-GCNs~\cite{Schlichtkrull2017Modeling}, has been proposed:
	\begin{equation}
	h_i^{(l+1)}=\sigma (\sum_{r \in \mathcal{R}}\sum_{j \in \mathcal{N}_i^r}\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+W_0^{(l)}h_i^{(l)})
	\end{equation}
	where $\mathcal{N}_i^r$ is the set of neighbor indices of node $i$ under relation $r \in \mathcal{R}$. $c_{i,r}$ is a normalization constant. $W_r^{(l)}$ is the weight matrix corresponding to relation $r$ in layer $l$ and $W_0^{(l)}$ denotes a special relation type, a single self-connection, to each node. ~\cite{Schlichtkrull2017Modeling} use a unique one-hot vector for each node in the graph as the input to the first R-GCN layer. And we improve this featureless approach with pre-defined node feature vectors in our RGCN entity alignment model .
	For R-GCN, multiple layers can be stacked to capture more neighboring characteristics across several relational steps.
	
	To solve the issue about the rapid growth in number of parameters with the number of relations when applying this propagation model, ~\cite{Schlichtkrull2017Modeling} provide two methods for regularizing the weights of R-GCN layers: basis- and block-diagonal-decomposition. In our entity alignment model, we choose the basis decomposition which will be introduced in detail in Section~\ref{section:rgcn}.
