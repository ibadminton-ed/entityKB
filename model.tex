\section{Our Approach}
	\label{section:app}
	Our work aims to develop an entity alignment model for any two arbitrary (heterogeneous) \KGs. . Without loss of generality, we
introduce our approach using two \KGs: $G_1 = (E_1,V_1,R_1,A_1,T_1)$ and $G_2 = (E_2,V_2,R_2,A_2,T_2)$ for entity alignment, where
$E,V,R,A,T$ represent entities, values, relations, attributes and triples respectively. 	We put $G_1$ and $G_2$ together in one large
graph $G$. We utilize pre-aligned entity pairs to train our models and then discover new equivalent entities. Figure~\ref{all} demonstrates
the overall architecture of our model. 	
	
	\begin{figure}[t!]
		\centering
			\includegraphics[width=0.8\linewidth]{figures/graph2.pdf}
			\caption{Overall architecture of our entity alignment model.}
			\label{all}
	\end{figure}
	
    \subsection{Base Model}
    Our approach is based on the recently proposed relational graph convolutional network (\RGCN)~\cite{Schlichtkrull2017Modeling}.
    \RGCN is an extension of Graph Convolutional Networks (\GCNs) that operate on local graph neighborhoods~\cite{Duvenaud2015Convolutional,Kipf2016Semi} to large-scale relational data.
    A \RGCN takes a set of adjacency matrices as input, and produces a new set of node features.
    Each input adjacency matrix describes the adjacency relationships among all nodes in the graph under each different relation.
    We choose to use \RGCN because it can model relational (directed and labeled) multi-graphs like \KGs.


 Our work improves \RGCNs in two ways. Firstly, we introduce highways to reduce the impact of noise.
    Secondly, we use automatically learned semantic features such as entity names and attributes to to help the \RGCN to better capture the
    subtle correlations between two \KGs.


 %   Our work improves the featureless approach of \RGCN with pre-defined node feature vectors.
%    We believe that in addition to the internal structures, the semantic information of entity names and the attribute information of entities can help \RGCN better embed \KGs.
%    Therefore, we incorporate the aforementioned information into the node features as part of the model inputs.


	
	
	\subsection{Our R-GCN-based Entity Alignment Model}
	\label{section:rgcn}	
  %  The input to our RGCN model are two parts. The first part is the node feature matrix $X^{(0)} \in \mathbb{R}^{N \times d^{(0)}}$ of $G$, where $N$ is the number of nodes and $d^{(0)}$ is the dimension of the input representations. We utilize predefined node features described in Section~\ref{subsection:Node Representations} to construct $X$ instead of using a featureless approach in \RGCNs~\cite{Schlichtkrull2017Modeling}.
%	The second part is the list of adjacency matrices $A=\{A_1,A_2,...,A_R |A_i \in \mathbb{R}^{N \times N} \}$, which describes the adjacency relationships among $N$ nodes under $R$ different relations. We extract $R_0$ original relations from knowledge graphs, then we add reverse relations in order to pass information from the opposite direction; and add the self loop to retain information of the node itself. These together compose $R=2R_0+1$ relations.
%	In each layer $l$, the input is $X^{(l)} = \{x^{(l)}_1,x^{(l)}_2,...,x^{(l)}_{N} |x^{(l)}_{i} \in \mathbb{R}^{d^{(l)}}\}$. The forward propagation is formulated as:

  	The input to our \RGCN model consists of two parts: (i) a node feature matrix that captures the semantic information such as the entity
  names and their attributes, and (ii) a list of adjacency matrices, which describes the adjacency relationships among nodes of a \KG.

  \FIXME{ZW: This section is so confusing. I have tried to rewrite it. Please double check.}

%
% We utilize predefined node features
% described in Section~\ref{subsection:Node Representations} to construct $X$ to capture the semantic information. 	The second part is

\subsubsection{i. Node representations}
	\label{subsection:Node Representations}
   We use a node feature matrix, $X^{(0)} \in \mathbb{R}^{N \times d^{(0)}}$ of $G$, to encode the semantic information of $N$  nodes in a $d^{(0)}$ dimensional input representation vector.
   This input directs the \RGCN to draw attention to the entity names and attributes, which are shown to be useful in entity
   alignment (Section~\ref{sec:motivation}). We stress that this node feature matrix is automatically initialized and updated during training.

	
	\eparagraph{Semantic information}
	\label{wordvector}
	Intuitively, if two entities can be linked together, their names in different \KGs should have similar semantics (e.g., the \emph{XinJiang} entity in Section~\ref{sec:motivation}).
    Our work exploits this observation to improve the quality of network features by using re-trained word embeddings to encode the semantic
    information entity names. This is achieved by applying a word2vec model to generate word embeddings from training entity names.
    %We use an open-source word2vec implementaion\footnote{https://code.google.com/archive/p/word2vec} to generate word embeddings.

	
	\eparagraph{Attribute information}
    In addition to entity names, we also consider entity attributes for node representations.
	Our current implementation distinguishes four types of attributes, i.e., \emph{Integer}, \emph{Double}, \emph{Date} and \emph{String}
(as default), but other data types can be added into the model.
	%In this paper, we only consider the first three types, i.e., Integer, Double and Date.
%	We overlook String type values by reason of their complexity and heterogeneity in different \KGs.
%	
	We construct normalized attribute vector for each entity. The dimension of an attribute vector for a given type equals to the number of distinct values of that type.
	The elements in an entity's attribute vector equal to the normalized values of the corresponding attributes. \FIXME{ZW: This sentence is confusing. Please consider rephrasing.}
    Because not all entities contain all the attribute types, we pad missing attributes with a value of 0 to form a constant-sized  vector.

 \subsubsection{ii. Adjacency relation}
 Like standard \GCNs, we extract node relations from an adjacency matrix of neighboring nodes in a graph.
 In our case, the adjacency matrices, $A=\{A_1,A_2,...,A_R |A_i \in \mathbb{R}^{N
 \times N} \}$, describe the adjacency relationships among $N$ nodes under $R$ different relations of graph $G$.

 To construct an adjacency matrix, $A_r$, for relation $r$ we follow a number of steps. We first extract $R_0$ original
 relations from \KGs, then we add reverse relations in order to pass information from the opposite direction, before we back-propagate the
 information to a network node through a self-referencing loop (for retaining the node's information). Applying these steps results in $R=2R_0+1$ relations. 	
 \FIXME{ZW: We either need a diagram or link this paragraph to Figure~\ref{all}}

 \eparagraph{Calculate the embedding matrix}
 Our approach stacks two \RGCNs as depicted in Figure~\ref{all}.
 The input for layer $l$ in each of our \RGCNs is a node feature vector, $X^{(l)}$, where  $X^{(l)} =\{x^{(l)}_1,x^{(l)}_2,...,x^{(l)}_{N}
 |x^{(l)}_{i} \in \mathbb{R}^{d^{(l)}}\}$. Each element of the node feature vector, $x_i$, is updated using forward propagation, and we stack the output of each \RGCN, $x_i^{(l+1)}$
 , to form an embedding matrix, $X^{(l+1)} \in \mathbb{R}^{N \times d^{(l+1)}}$.

   Specifically, the forward propagation of $x_i^{(l+1)}$ of a \RGCN is formulated as:
	\begin{equation}
	x_i^{(l+1)}=\mathrm{ReLU} (\sum\limits_{r \in R}\sum\limits_{j \in N_i^r}\frac{1}{|N_i^r|}W_r^{(l)}x_j^{(l)})
	\end{equation}
%
%
where $N_i^r$ is the set of neighbor indices of node $i$ under relation $r$, according to normalized adjacency matrix $\hat A_r$; and $\hat
A_r$ is an approximate of spectral convolutions on $A^r$, introduced by ~\cite{Kipf2016Semi}:
	\begin{equation}
	\hat A_r=\hat D_r^{- \frac{1}{2}}(A_r+I)\hat D_r^{- \frac{1}{2}}
	\end{equation}
	where $(\hat D_r)_{jj}=\sum_k(A_r+I)_{jk}$.

	
	\eparagraph{Calculate the weight matrix} For each relation $r$, we construct a weight matrix, $W_r^{(l)} \in \mathbb{R}^{d^{(l+1)}
\times d^{(l)}}$ for the stacked \RGCNs. As there are often thousands types of relations in \KGs, there will be a large amount of
parameters to train, and as a result the model is likely to overfit. To avoid overfiting, we employ the basis decomposition, which is
introduced in ~\cite{Schlichtkrull2017Modeling}, to regularize the weights:
	\begin{equation}
	W_r^{(l)}=\sum\limits_{b=1}^B a_{rb}^{(l)}V_b^{(l)}
	\end{equation}
	where $V_b^{(l)} \in \mathbb{R}^{d^{(l+1)} \times d^{(l)}}$ and $a_{rb}^{(l)}$ is the coefficient of matrix $V_b^{(l)}$ for relation $r$.

As a departure from general \GCNs, our approach applies relation-specific transformations to an edge depending on its type and direction.
Later in the paper, we show that this strategy can better capture the multi-relational data characteristics of real-world \KGs. \FIXME{ZW:
Why do we need this paragraph? I don't know where to put it.}

	
	
%	Since it is the first attempt, we only consider the numerical part of a value, regardless of the unit, that is, we do not insist on normalizing \emph{1.80m} and \emph{180cm}.
%	We leave this for future work.


	\subsubsection{Noise reduction}
	\label{section:hgcn}
	While stacking RGCN layers makes our model capable of learning more neighborhood information from several relational steps, it may as
well bring noise from the exponentially increasing neighbors. To reduce the effect of noise and ensure effective spread of more informative
and discriminative neighborhood information, we add layer-wise gates similar to highway networks~\cite{Srivastava2015Highway} to our RGCN
entity alignment model. ~\cite{Rahimi2018Semi} have successfully introduced highway gates to \GCNs~\cite{Kipf2016Semi} to solve the user
geolocation problem. We introduce layer-wise highway gates to our RGCN model to finally get Highway RGCN (HRGCN) model and the output of a
HRGCN layer is computed as:
	\begin{equation}
	\begin{split}
	&T(x^{(l)})=\sigma(W_T^{(l)}x^{(l)}+b_T^{(l)}) \\
	&x^{(l+1)}=x^{(l+1)} \cdot T(x^{(l)})+x^{(l)} \cdot (1-T(x^{(l)}))
	\end{split}
	\end{equation}
	where $\sigma$ indicates the sigmoid activation function, $\cdot$ is element-wise multiplication, $W_T^{(l)} \in \mathbb{R}^{d^{(l+1)} \times d^{(l)}}$ and $b_T^{(l)} \in \mathbb{R}^{d^{(l)} \times 1}$ are the weight matrix and bias vector of transform gate $T(x^{(l)})$.
	
	\subsection{Alignment Prediction}
	After HRGCN layers, we get the hidden representations $\bar{X}$ of all nodes in both \KGs. We measure the similarity between $e_1$ in $G_1$ and $e_2$ in $G_2$ by the distance between their hidden representations:
	\begin{equation}
	\label{d}
	d(e_1,e_2)=|\bar{x}_{e_1}-\bar{x}_{e_2}|
	\end{equation}
	, where $|\cdot|$ indicates the $l_1$ norm. The distance for equivalent entities is expected to be smaller than non-equivalent ones. In our experiments, for a entity $e_1$ in $G_1$, we computes the distances between $e_1$ and all the entities in $G_2$. The alignment process can also be reversed, i.e. from $G_2$ to $G_1$. Since, we report the results of both directions of entity alignment in our experiments.
	
	A set of pre-aligned entity pairs $\mathbb{L}$ and the set of negative pairs $\mathbb{L'}$  constructed by corrupting $(p, q)$, i.e. replacing $p$ or $q$ with a randomly chosen entity in $G_1$ or $G_2$ are used for training. To maximize the distance between positive and negative instances, we use the margin-based loss function:
	\begin{equation}
	L=\sum\limits_{(p,q)\in \mathbb{L}}\sum\limits_{(p',q')\in \mathbb{L'}}\mathrm{max}\{0,d(p,q)-d(p',q')+\gamma\}
	\end{equation}
	$\gamma > 0$ is a margin hyper-parameter separating positive and negative entity alignments.
	
	\subsection{Combination of Semantic Embedding and Attribute Embedding}
	As we introduced in Section~\ref{subsection:Node Representations}, there are two methods to initialize the input node feature vectors.
	When we leverage the pre-trained word embeddings to initialize the node feature vectors, we call HRGCNs acting semantic embedding.
	And when the nodes are initialized by attribute vectors, we call HRGCNs performing attribute embedding.
	Following the same architecture in Figure~\ref{all}, we do semantic embedding and attribute embedding for $G$ respectively.
	
	We integrate semantic embedding and attribute embedding by defining a combined distance $D$ for aligning entities:
	\begin{equation}
		D(e_1,e_2)=\frac{1}{m}[\omega d_s(e_1,e_2)+(1-\omega)d_a(e_1,e_2)]
	\end{equation}

	where $m$ is the dimension of the new node features produced by HRGCNs.
	In our experiments, we set the output dimensions of two models (one for semantic embedding and the other one for attribute embedding) to be the same.
	$\omega$ is a hyper-parameter. $d_s$ and $d_a$ are the distance computed by Eq.~\ref{d} according to semantic embedding and attribute embedding, respectively.
	
