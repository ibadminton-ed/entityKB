\section{Our Approach}
	\label{section:app}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\textwidth]{figures/overview.pdf}\\
  \caption{Overview of our approach}\label{fig:overview}
\end{figure}

\subsection{Overview}
Figure~\ref{fig:overview} gives a high-level overview of our approach. To align entities from two \KGs, we first manually label some entity
pairs from the target \KGs to construct a training dataset. The training dataset contains both aligned and unaligned entity pairs. Next, we
extract graph information from the \KGs, which is used to capture the semantics and attributes of entities as well as the relation of
neighboring nodes (Section~\ref{section:rgcn}). Then, we learn a neural network (Section~\ref{sec:model}) to take in the graph information
(organized as matrices) and produce a representation vector for each entity of the input \KGs. The network output is optimized in a way
that two aligned entities in the training data are similar to each other in the feature space. With this entity representation in place, we
can calculate the distance between any two entity representations to determine if the entities should be aligned
(Section~\ref{prediction}).

	

	
    \subsection{Knowledge Graph Representation Model\label{sec:model}}
    Our model for generating \KG representations  is based on the recently proposed relational graph convolutional network (\RGCN)~\cite{Schlichtkrull2017Modeling}.
    The \RGCN is an extension of Graph Convolutional Networks (\GCNs) that operate on local graph neighborhoods~\cite{Duvenaud2015Convolutional,Kipf2016Semi} to large-scale relational data.
   % A \RGCN layer takes a set of adjacency matrices as input, and produces a new set of node features.
%    Each input adjacency matrix describes the adjacency relationships among all nodes in the graph under each different relation.
    We choose to use \RGCN because it can model relational (directed and labeled) multi-graphs like \KGs.

 Our work improves \RGCNs in two ways. Firstly, we use automatically learned semantic features such as entity names and attributes to
 help the \RGCN better capture the subtle correlations of entities from two different \KGs (Section~\ref{section:rgcn}.i).
    Secondly, we introduce highway gates to reduce the impact of noise (Section~\ref{section:hgcn}).


 %   Our work improves the featureless approach of \RGCN with pre-defined node feature vectors.
%    We believe that in addition to the internal structures, the semantic information of entity names and the attribute information of entities can help \RGCN better embed \KGs.
%    Therefore, we incorporate the aforementioned information into the node features as part of the model inputs.


	



	
	\subsection{Knowledge Graph Information}
	\label{section:rgcn}	
  %  The input to our RGCN model are two parts. The first part is the node feature matrix $X^{(0)} \in \mathbb{R}^{N \times d^{(0)}}$ of $G$, where $N$ is the number of nodes and $d^{(0)}$ is the dimension of the input representations. We utilize predefined node features described in Section~\ref{subsection:Node Representations} to construct $X$ instead of using a featureless approach in \RGCNs~\cite{Schlichtkrull2017Modeling}.
%	The second part is the list of adjacency matrices $A=\{A_1,A_2,...,A_R |A_i \in \mathbb{R}^{N \times N} \}$, which describes the adjacency relationships among $N$ nodes under $R$ different relations. We extract $R_0$ original relations from knowledge graphs, then we add reverse relations in order to pass information from the opposite direction; and add the self loop to retain information of the node itself. These together compose $R=2R_0+1$ relations.
%	In each layer $l$, the input is $X^{(l)} = \{x^{(l)}_1,x^{(l)}_2,...,x^{(l)}_{N} |x^{(l)}_{i} \in \mathbb{R}^{d^{(l)}}\}$. The forward propagation is formulated as:


    Figure~\ref{all} depicts the overall architecture of our \RGCN-based \KG representation model. 	
  	The input to our model consists of two parts: (i) a node feature matrix that captures the semantic information such as the entity
  names and their attributes, and (ii) a list of adjacency matrices, which describes the adjacency relationships among nodes of a \KG.


\subsubsection{Notations}

Without loss of generality, we introduce our model using two \KGs: $G_1 = (E_1,V_1,R_1,A_1,T_1)$ and $G_2 = (E_2,V_2,R_2,A_2,T_2)$ for
entity alignment, where $E,V,R,A,T$ represent entities, values, relations, attributes and triples respectively. 	We put $G_1$ and $G_2$
together in one large graph $G$. We utilize pre-aligned entity pairs to train our model and then discover latent aligned entities.

%


% We utilize predefined node features
% described in Section~\ref{subsection:Node Representations} to construct $X$ to capture the semantic information. 	The second part is

\subsubsection{i. Node representations}
	\label{subsection:Node Representations}
   We use a node feature matrix, $X^{(0)} \in \mathbb{R}^{N \times d^{(0)}}$ of $G$, to encode the semantic information of $N$  nodes in a
   $d^{(0)}$ dimensional input vector. This input directs \RGCN to draw attention to the entity names and attributes, which are shown to be
   useful in entity alignment (Section~\ref{sec:motivation}). This node representation can be used to encode either the entity names
   (semantic information) or attributes, or a combination of both. In \FIXME{Section~\ref{}}, we provide a evaluation of different node
   representation strategies.


	
	\eparagraph{Semantic information}
	\label{wordvector}
	Intuitively, if two entities can be linked together, their names in different \KGs should have similar semantics (e.g., the \emph{XinJiang} entity in Section~\ref{sec:motivation}).
    Our work exploits this observation to improve the quality of the network features by using pre-trained word embeddings to encode the semantic
    information of entity names.
    %This is achieved by applying a word2vec model to generate word embeddings from training entity names.
    %We use an open-source word2vec implementaion\footnote{https://code.google.com/archive/p/word2vec} to generate word embeddings.

	
	\eparagraph{Attribute information}
    In addition to entity names, we also consider entity attributes for node representations.
	Our current implementation distinguishes four types of attributes, i.e., \emph{Integer}, \emph{Double}, \emph{Date} and \emph{String}
    (as default), but other data types can be added into the model. When examining attributes of \emph{Integer}, \emph{Double}, and \emph{Date} types,
    we also record the range of values that can be taken by each type.
	%In this paper, we only consider the first three types, i.e., Integer, Double and Date.
%	We overlook String type values by reason of their complexity and heterogeneity in different \KGs.
%	
	For for each entity within two \KGs, We construct A normalized attribute vector. The dimension of the vector equals to the number of distinct attributes of the target attribute types.
	Each element in the vector equals to the normalized value of the corresponding attribute.
    Because not all entities contain all the attributes, we pad missing attributes with a value of 0 to form a constant-sized vector.

 \subsubsection{ii. Neighboring node relation}
 Like standard \RGCNs, we capture the relation of graph neighboring nodes using an adjacency matrix.
 In our case, the adjacency matrices, $A=\{A_1,A_2,...,A_R |A_i \in \mathbb{R}^{N
 \times N} \}$, describe the adjacency relationships among $N$ nodes under $R$ different relations of graph $G$.

 To construct an adjacency matrix, $A_r$, for relation $r$ we follow a number of steps. We first extract $R_0$ original
 relations from \KGs, then we add reverse relations in order to pass information from the opposite direction. And for retaining the node's information, we also add a self-referencing loop to each node. Applying these steps results in $R=2R_0+1$ relations. As shown in Figure~\ref{all}, original relations, reverse relations and the self loops all participate in the updating of node features.

 \eparagraph{Calculate the embedding matrix}
 The input for layer $l$ in each of our stacked \RGCNs is a node feature matrix, $X^{(l)}$, of $N$ vectors, where  $X^{(l)} =\{x^{(l)}_1,x^{(l)}_2,...,x^{(l)}_{N}
 |x^{(l)}_{i} \in \mathbb{R}^{d^{(l)}}\}$. Each node feature vector, $x_i$, is updated using forward propagation, and we stack the output vector for each node, $x_i^{(l+1)}$
 , to form an embedding matrix, $X^{(l+1)} \in \mathbb{R}^{N \times d^{(l+1)}}$.

   The forward propagation of $x_i^{(l+1)}$ of a \RGCN is formulated as:
	\begin{equation}
	x_i^{(l+1)}=\mathrm{ReLU} (\sum\limits_{r \in R}\sum\limits_{j \in N_i^r}\frac{1}{|N_i^r|}W_r^{(l)}x_j^{(l)}),
	\end{equation}
where $N_i^r$ is the set of neighbor indices of node $i$ under relation $r$, according to normalized adjacency matrix $\hat A_r$; and $\hat
A_r$ is an approximate of spectral convolutions on $A^r$, introduced by ~\cite{Kipf2016Semi}:
	\begin{equation}
	\hat A_r=\hat D_r^{- \frac{1}{2}}(A_r+I)\hat D_r^{- \frac{1}{2}},
	\end{equation}
	where $(\hat D_r)_{jj}=\sum_k(A_r+I)_{jk}$.

	
	\eparagraph{Calculate the weight matrix} For each relation $r$, we construct a weight matrix, $W_r^{(l)} \in \mathbb{R}^{d^{(l+1)}
\times d^{(l)}}$ for the stacked \RGCNs. As a \KG often has over thousands of types of relations, there will be a large amount of network
parameters to learn over, which is likely to lead to overfiting. To minimize the impact of the large parameter size, we employ the basis
decomposition method~\cite{Schlichtkrull2017Modeling} to regularize the weights:
	\begin{equation}
	W_r^{(l)}=\sum\limits_{b=1}^B a_{rb}^{(l)}V_b^{(l)},
	\end{equation}
	where $V_b^{(l)} \in \mathbb{R}^{d^{(l+1)} \times d^{(l)}}$ and $a_{rb}^{(l)}$ is the coefficient of matrix $V_b^{(l)}$ for relation $r$.
	
	
%	Since it is the first attempt, we only consider the numerical part of a value, regardless of the unit, that is, we do not insist on normalizing \emph{1.80m} and \emph{180cm}.
%	We leave this for future work.


	\subsection{Noise control}
	\label{section:hgcn}
	While stacking \RGCN layers enhances our model's capability in learning neighborhood information from relational steps, it may as well
bring noise from the exponentially increasing number of neighbors. To ensure effective spread of informative and discriminative
neighborhood information, we add layer-wise gates similar to highway networks~\cite{Srivastava2015Highway} to our \RGCN-based model.
%~\cite{Rahimi2018Semi} have successfully introduced highway gates to \GCNs~\cite{Kipf2016Semi} to solve the user geolocation problem.
We use layer-wise highway gates to build a Highway \RGCN (\HRGCN) model where the output of a \HRGCN layer is computed as:
	\begin{equation}
	\begin{split}
	&T(x^{(l)})=\sigma(W_T^{(l)}x^{(l)}+b_T^{(l)}), \\
	&x^{(l+1)}=x^{(l+1)} \cdot T(x^{(l)})+x^{(l)} \cdot (1-T(x^{(l)})),
	\end{split}
	\end{equation}
	where $\sigma$ indicates the sigmoid activation function, $\cdot$ is element-wise multiplication, $W_T^{(l)} \in \mathbb{R}^{d^{(l+1)}
\times d^{(l)}}$ and $b_T^{(l)} \in \mathbb{R}^{d^{(l)} \times 1}$ are the weight matrix and bias vector of transform gate $T(x^{(l)})$.
    In Section~\ref{overall}, we show that the highway gates significantly boost the performance of our \RGCN-based model.
	
	\subsection{Alignment Prediction\label{prediction}}
	After \HRGCN layers, we get the hidden representations $\bar{X}$ of all nodes in both \KGs. We then measure the similarity between of two
nodes, $e_1$ and $e_2$ from $G_1$ and $G_2$, respectively. The similarity is determined by computing the distance between the hidden
representations of two nodes:
	\begin{equation}
	\label{d}
	d(e_1,e_2)=||\bar{x}_{e_1}-\bar{x}_{e_2}||_{L_1}.
	\end{equation}
	Since we expect the distance for latent aligned entities to be smaller than the non-equivalent counterparts and hope the distance between positive and negative aligned entity pairs can be separated by a large margin, we utilize a margin-based score function as the training objective, defined as:
	\begin{equation}
	L=\sum\limits_{(p,q)\in \mathbb{L}}\sum\limits_{(p',q')\in \mathbb{L'}}\mathrm{max}\{0,d(p,q)-d(p',q')+\gamma\},
	\end{equation}
	where $\gamma > 0$ is a margin hyper-parameter; $\mathbb{L'}$ stands for the negative alignment set of $\mathbb{L}$, defined as follows:
	\begin{equation}
	\mathbb{L'}=\{(p',q)|p'\in E_1\}\cup\{(p,q')|q'\in E_2\}, (p,q)\in \mathbb{L'}.
	\end{equation}
	This indicates one of two entities in an aligned entity pair is randomly replaced by others.
	
	%In our experiments, for a entity $e_1$ in $G_1$, we compute the distances between $e_1$ and all the entities in $G_2$. The alignment process can also be reversed, i.e., from $G_2$ to $G_1$. In Section~\ref{sec:results}, we report the results of both directions of entity alignment.

	
	\subsection{Combination of Semantic and Attribute Embeddings}
    Recall that the node feature vector can be initialized from either semantics (i.e., entity names) or attributes (Section~\ref{subsection:Node
    Representations}). For combining the semantic and attribute embeddings, the most straightforward way is to simply concatenate a pre-trained word vector and an attribute vector as an input node feature vector. Another idea is to apply two stacked \HRGCNs to respectively perform a semantic or an attribute embedding.
	%When we leverage the pre-trained word embeddings to initialize the node feature vectors, we call HRGCNs acting semantic embedding.
%	And when the nodes are initialized by attribute vectors, we call HRGCNs performing attribute embedding.
%	Following the same architecture in Figure~\ref{all}, we do semantic embedding and attribute embedding for $G$ respectively.
%
	For the latter, we define a combined distance to integrate semantic embedding and attribute embedding:
	\begin{equation}
		D(e_1,e_2)=\frac{1}{m}[\omega d_s(e_1,e_2)+(1-\omega)d_a(e_1,e_2)]
	\end{equation}

	where $m$ is the dimension of the new node features produced by \HRGCNs, $\omega$ is a hyper-parameter. $d_s$ and $d_a$ are the distance computed by Eq.~\ref{d} according to semantic embedding and attribute embedding, respectively.
	In our experiments, we set the output dimensions of two models (one for semantic embedding and the other one for attribute embedding) to be the same.
	
	
