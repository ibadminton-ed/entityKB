\section{Experimental Setup}
	Our experiments are designed to investigate whether \RGCN-based models can better represent an entity with its highly multi-relational
structure information compared to \KG embedding-based methods, and whether highway gates can effectively capture discriminative neighboring
information by controlling the balance of how much neighborhood information should be passed to a node in \RGCN.

\subsection{Datasets}
\label{subsection:datasets}
	We use two real-world datasets: DBP15K from~\cite{sun2017cross} and a Chinese language dataset constructed by ourselves. The latter
dataset is built using data extracted from Wikipedia and Baidu Baike (the largest Chinese-language web-based encyclopedia), which is
referred as Wikipedia-Baidu dataset (WBD)\footnote{Dataset available at: [\emph{url redacted for double-blind review}]}.
	
	\cparagraph{DBP15K} This is a cross-lingual dataset built upon DBpedia.
	It has a series of subsets, each of which contains data from two \KGs in different languages and provides 15,000 pre-aligned entity pairs.
	We use the French-English subset of DBP15K, because it is considered to be the most challenging subset for entity alignment~\cite{sun2017cross}.
     Table~\ref{dbp} shows the statistics of the selected subset.
	
		\begin{table}
		\centering
		\scriptsize
		\begin{tabular}{c|ccccc}
			\toprule
			&\bf  Entities &\bf  Relations &\bf  Attributes &\bf  Rel. triples &\bf  Attr. triples \\
			\midrule
			French & 66,858 & 1,379 & 4,547 & 192,191 & 528,665 \\
			English & 105,889 & 2,209 & 6,422 & 278,590 & 576,543 \\
			\bottomrule
		\end{tabular}
		\caption{The statistics for $DBP15K_{FR-EN}$.}
		\label{dbp}
	\end{table}
	
%    For the Chinese dataset, we extracted data from two web-based encyclopedia sources: Wikipedia and Baidu Baike (which is the largest Chinese-language web-based encyclopedia).
%    The two sources contain rich information and are widely used to constructed \KGs.
%	
%	\cparagraph{Raw Wiki dataset}
%	We used the Chinese version of the Wikipedia dump\footnote{https://dumps.wikimedia.org/zhwiki} released on June 2, 2015.
%    From the dump, we extracted over one million entities and three million $\langle$subject, predicate, object$\rangle$ triples from the infobox (i.e., a fixed-format table that summarizes some unifying aspect of a Wikipedia article) to form the Wiki \KG.
%	
%	\cparagraph{Raw Baidu dataset} From Baidu Baike, we extracted 11.55 million entities and 39.2 million structured triples from the infobox on each page to construct the Baidu \KG.
%	The data were collected from Baidu Baike between February 2018 and July 2018.
	
	\cparagraph{WBD} We manually aligned 16,969 randomly chosen entity pairs from the Chinese version of the Wikipedia dump (released on June 2, 2015) and Baidu Baike data (collected between February 2018 and July 2018).
	To construct the dataset, we first randomly  selected an aligned entity pair and then extracted relation and attribute triples for selected entities.
	Table~\ref{dataset} gives the statistics of this dataset.
	



%dataset WBD for . Since both of the two KGs mentioned above are constructed from data that we crawled from web encyclopedias, there are no ready-made aligned entity pairs. Therefore, we manually aligned a portion of entities as the seed data.

	\begin{table}
	\centering
	\scriptsize
	\begin{tabular}{c|ccccc}
		\toprule
		&\bf  Entities &\bf  Relations &\bf  Attributes &\bf  Rel. triples &\bf  Attr. triples \\
		\midrule
		Wiki & 25,139 & 1,301 & 8,332 & 87,775 & 211,949 \\
		Baidu & 32,101 & 2,262 & 20,263 & 143,228 & 303,116 \\
		\bottomrule
	\end{tabular}
	\caption{The statistics for WBD.}
	\label{dataset}
\end{table}


	\subsection{Competitive Approaches}
    \cparagraph{Baselines}
	We build two \GCN-based models as our baselines: a \GCN and a \HGCN, which respectively replace the \RGCN layers of our \RGCN and \HRGCN with standard \GCN layers~\cite{Kipf2016Semi} to embed the structure information of two \KGs.
	The two \GCN-based models combine the semantic and attribute embeddings with $\omega=0.95$ in the combined distance.
	In our experiments, we stack two \GCN layers, where each \GCN contains 64 hidden units.
	To avoid over-fitting, we apply L2 regularization with $\lambda=0.00001$ and utilize dropout with $\mathrm{dropout rate}=0.1$.
	These hyperparameters were found to give the best overall performance in our experiments.



    \cparagraph{Prior methods}
    On the $DBP15K_{FR-EN}$ dataset, we compare our \HRGCN model against JE~\cite{hao2016joint}, MTransE~\cite{chen2016multilingual} and JAPE~\cite{sun2017cross}.
    These competitive models have been tuned and tested on this dataset.
	On our WBD dataset, we compare our approach against two JE and ITransE~\cite{zhu2017iterative}. ITransE requires all relations to be
shared between two \KGs. This strong assumption, however, cannot be satisfied on heterogeneous \KGs. To provide a fair comparison, we build
ITransE$'$, a variant of ITransE which does not require the \KGs to have the same relations. 	


	
	\cparagraph{Variants of our model} We evaluate our approach on three variant implementaions: two-layered Highway \RGCN (\HRGCN),
two-layered no-highway \RGCN and \HRGCN (w/o X) which does not use the predefined feature vectors mentioned in Section~\ref{subsection:Node
Representations} as the input to the \HRGCN. For all variants of our model, we use \RGCN with 16 hidden units for each layer and $B = 80$
for basis function decomposition. We set $\omega=0.95$ in the combined distance for semantic and attribute embeddings, and apply L2
regularization with $\lambda=5e-4$ to avoid overfitting.

 %  Different from WBD dataset, we use RGCN with 32 hidden units for each layer and $B = 100$ for basis function decomposition.
%	The hyper-parameter $\omega$ in the combined distance is set to 0.9.
    %ZW: I don't know why you use different hidden units for different datasets. But this doesn't look good, so I comment it out.

	
	



	\subsection{Evaluation Methodology}
    	For the $DBP15K_{FR-EN}$ dataset, we use the same split of gold standards as in~\cite{sun2017cross}: 30\% for training and 70\% for
    testing.
	For semantic embedding, we first directly utilize Google Translate to translate French entities and relations into English.
	Then, we use pre-trained English word vectors \emph{glove.840B.300d}~\footnote{http://nlp.stanford.edu/projects/glove/} to construct the input node feature vectors.
	For attribute embedding, we select 180 high-frequency attributes from each \KG and generate 180-dimensional attribute vectors.	


	For the WBD dataset, we break the gold standards for two parts with equal-size, and use one for training and the other for testing.
	As mentioned in Section~\ref{wordvector}, we utilize pre-trained 100-dimensional Chinese word vectors.
	By counting the frequency of attributes appearing in each \KG of the WBD dataset, we select 63 high-frequency attributes from each \KG to construct 63-dimensional attribute vectors.
     To learn the word vectors to capture the semantics of entity names for the node representation (see Section~\ref{subsection:Node
    Representations}), we use sentences from  the Baidu dataset. We use a window size of 5 and a threshold of 20 for downsampling the frequent words
    for the word2vec model.
	

	
	We use Hits@k to assess the performance of all the approaches.
	This metric is widely used for evaluating entity alignment~\cite{hao2016joint,chen2016multilingual,sun2017cross,zhu2017iterative} by measuring the proportion of correctly aligned entities ranked in the top k.
	All the neural network based models are trained using the Adam optimizer~\cite{Kingma2014Adam} for a maximum of 200 epochs with a learning rate
    of 0.01.  Moreover, all models are initialized using Glorot initialization~\cite{Glorot2010Understanding}.
	
