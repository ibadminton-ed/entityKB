\section{Experimental Setup}
	Our experiments are designed to investigate whether \RGCN-based models can better represent an entity with its highly multi-relational
structure information compared to \KG embedding-based methods, and whether highway gates can effectively capture discriminative neighboring
information by controlling the balance of how much neighborhood information should be passed to a node in \RGCN.

\subsection{Datasets}
	We evaluate our on two Chinese \KGs extracted from two web-based encyclopedia sources: Wikipedia and Baidu Baike (which is the largest Chinese-language web-based encyclopedia). The two sources contain a rich of information and are widely used to constructed \KGs.
	
	\cparagraph{Wiki}
	We use the Chinese version of the Wikipedia dump\footnote{https://dumps.wikimedia.org/zhwiki} released on June 2, 2015.
    From the dump, we extract over one million entities and three million $\langle$subject, predicate, object$\rangle$ triples from the infobox (i.e., a fixed-format table that summarizes some unifying aspect of a Wikipedia article) from each page to form the Wiki \KG.
	
	\cparagraph{Baidu} From Baidu Baike, we extract 11.55 million entities and 39.2 million structured triples from the infobox on each page to construct the Baidu \KG. The data were collected from Baidu Baike between \FIXME{February 2018 and July 2018?}.

	
	\cparagraph{Gold standards} From the aforementioned two \KGs, we manually aligned 16,969 entity pairs as the gold standards of entity alignment, and built a new dataset, called WBD, for entity alignment in Chinese KGs. \FIXME{ZW: What is WBD used for???}
	

%dataset WBD for . Since both of the two KGs mentioned above are constructed from data that we crawled from web encyclopedias, there are no ready-made aligned entity pairs. Therefore, we manually aligned a portion of entities as the seed data.
	\begin{table}
		\centering
		\begin{tabular}{llll}
			\toprule
			& Wiki & Baidu & Total \\
			\midrule
			\#Entities & 25,139 & 32,101 & 57,240 \\
			\#Relations & 1,301 & 2,262 & 3,563 \\
			\#Attributes & 8,332 & 20,263 & 28,595 \\
			\#Triples & 147,139 & 446,344 & 593,483 \\
			%\multicolumn{4}{l}{\#Alignment Entity Pairs: 8000} \\
			\bottomrule
		\end{tabular}
		\caption{The statistics for WBD.}
		\label{dataset}
	\end{table}
	
	\subsection{Competitive Approaches}
	We compare our approach against two state-of-the-art entity alignment methods~\cite{hao2016joint,zhu2017iterative} and alternative \GCN-based models, described as follows.

	\cparagraph{JE} As shown in Table~\ref{seed}, JE~\cite{hao2016joint} requires the similar seed alignments, pre-aligned entities, as our model, which also performs well among all existing embedding-based models. We use our best effort to implement this model as they do not release any source code or software currently.
	
	\cparagraph{ITransE$'$} ITransE~\cite{zhu2017iterative} is another representative embedding-based methods for entity alignment. However, ITransE requires all relations to be shared between two KGs. Since our WBD dataset was built based on two completely heterogeneous KGs, sharing same relations is unrealistic for our dataset. Thus we build ITransE$'$, a variant of ITransE which does not use same relations between KGs.
	
	\cparagraph{GCNs} We also build a GCN-based model as our baseline which utilize GCNs~\cite{Kipf2016Semi} to embed the structure information of two \KGs and also takes our predefined node features as input node feature matrix. In our experiments, we stack two \GCN layers, where each \GCN contains 64 hidden units. To avoid over-fitting, we apply L2 regularization with $\lambda=0.00001$ and utilize dropout with $\mathrm{dropout rate}=0.1$.
    These hyperparameters were found to give the best overall performance in our experiments.
	
	\cparagraph{Variants of our model} Our full model is two-layered Highway RGCN (HRGCN). We also build a no-highway variant, two-layered \RGCN. We build another variant HRGCN (w/o X) which does not use the predefined feature vectors mentioned in Section~\ref{subsection:Node Representations} as the input to our model.
	For all variants of our model, we use RGCN with 16 hidden units for each layer and $B = 80$ for basis function decomposition. We apply L2 regularization with $\lambda=5e-4$ to avoid overfitting.



	\subsection{Evaluation Methodology}
	We break the gold standards for two parts with equal-size, and use one for training and the other for testing. We use Hits@k to assess the performance of all the approaches. This metric is widely used for evaluating entity alignment~\FIXME{\cite{}} by measuring the proportion of correctly aligned entities ranked in the top k.
    As mentioned in Section~\ref{wordvector}, we utilize pre-trained 100-dimensional word vectors. By counting the frequency of attributes appearing in each \KG of the WBD dataset, we select 63 high-frequency attributes from each \KG to construct 63-dimensional attribute vectors.


    All the neural network based models are trained using the Adam optimizer~\cite{Kingma2014Adam} for maximum of 200 epochs with a learning rate
    of 0.01. Furthermore, all the models are initialized using Glorot initialization~\cite{Glorot2010Understanding}.
	
