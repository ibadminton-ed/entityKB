\section{Experimental Setup}
	Our experiments\footnote{Code and data available at: [\emph{url redacted for blind review}]} are designed to investigate (1) if \RGCN-based models can better represent an entity compared to existing \KG
embedding-based methods, and (2) whether highway gates can effectively capture discriminative neighboring information by controlling how
much neighborhood information should be passed to a node in an \RGCN.

\subsection{Datasets}
\label{subsection:datasets}
	We use two real-world datasets: DBP15K from~\cite{sun2017cross} and a Chinese language dataset constructed by ourselves. The latter
dataset is built using data extracted from Wikipedia and Baidu Baike (the largest Chinese-language web-based encyclopedia), which is
referred as Wikipedia-Baidu dataset (WBD).
	
	\cparagraph{DBP15K} This is a cross-lingual dataset built upon DBpedia.
	It has a series of subsets, each of which contains data from two \KGs in different languages and provides pre-aligned entity pairs.
	We use the French-English subset of DBP15K, because it is considered to be the most challenging subset for entity alignment~\cite{sun2017cross}. Table~\ref{dbp} shows the statistics of the selected subset.
	
		\begin{table}
		\centering
		\scriptsize
		\begin{tabular}{l|rrrrr}
			\toprule
			&\bf  Entities &\bf  Relations &\bf  Attributes &\bf  Rel. triples &\bf  Attr. triples \\
			\midrule
			French & 66,858 & 1,379 & 4,547 & 192,191 & 528,665 \\
			English & 105,889 & 2,209 & 6,422 & 278,590 & 576,543 \\
			\bottomrule
		\end{tabular}
		\caption{Summary of the $DBP15K_{FR-EN}$ dataset.}
		\label{dbp}
	\end{table}
	
%    For the Chinese dataset, we extracted data from two web-based encyclopedia sources: Wikipedia and Baidu Baike (which is the largest Chinese-language web-based encyclopedia).
%    The two sources contain rich information and are widely used to constructed \KGs.
%	
%	\cparagraph{Raw Wiki dataset}
%	We used the Chinese version of the Wikipedia dump\footnote{https://dumps.wikimedia.org/zhwiki} released on June 2, 2015.
%    From the dump, we extracted over one million entities and three million $\langle$subject, predicate, object$\rangle$ triples from the infobox (i.e., a fixed-format table that summarizes some unifying aspect of a Wikipedia article) to form the Wiki \KG.
%	
%	\cparagraph{Raw Baidu dataset} From Baidu Baike, we extracted 11.55 million entities and 39.2 million structured triples from the infobox on each page to construct the Baidu \KG.
%	The data were collected from Baidu Baike between February 2018 and July 2018.
	
	\cparagraph{WBD} This dataset is constructed from the Chinese version of the Wikipedia dump (released on June 2, 2015), and Baidu Baike (collected between February 2018 and July 2018).
    Table~\ref{dataset} gives the statistics of this dataset. From this dataset, we manually aligned 16,969 entities as the gold standards.

   % From the the Chinese version
%of the Wikipedia dump (released on June 2, 2015), we extracted over 1 million entities and three million $\langle$subject, predicate,
%object$\rangle$ triples. From Baidu Baike (collected between February 2018 and July 2018), we extracted over 11.55 million entities and
%39.2 million structured triples. To construct the evaluation dataset used in our experiments, we manually aligned 16,969 randomly chosen entity pairs from both \KGs as the gold standards. Then for each of the aligned entity pairs, we extracted relation and attribute triples for two entities from two \KGs, respectively. Table~\ref{dataset} gives the statistics of this dataset. \FIXME{Check the texts against the table.}
%	



%dataset WBD for . Since both of the two KGs mentioned above are constructed from data that we crawled from web encyclopedias, there are no ready-made aligned entity pairs. Therefore, we manually aligned a portion of entities as the seed data.

	\begin{table}
	\centering
	\scriptsize
	\begin{tabular}{l|rrrrr}
		\toprule
		&\bf  Entities &\bf  Relations &\bf  Attributes &\bf  Rel. triples &\bf  Attr. triples \\
		\midrule
		Wikipedia & 25,139 & 1,301 & 8,332 & 87,775 & 211,949 \\
		Baidu & 32,101 & 2,262 & 20,263 & 143,228 & 303,116 \\
		\bottomrule
	\end{tabular}
	\caption{Summary of the WBD dataset.}
	\label{dataset}
\end{table}


	\subsection{Competitive Approaches}
    \cparagraph{Baselines}
	We develop three baselines: a vanilla \HRGCN without highway gates, a \GCN and a \GCN with highway gates (\HGCN). The two latter \GCN-based models are built upon the model presented at~\cite{Kipf2016Semi}.
    All the baseline models use both the semantic and attribute embeddings as the node representation (Section~\ref{sec:combine}).

	%The baseline models combine the semantic and attribute embeddings with $\omega=0.95$ in the combined distance.
	%In our experiments, we stack two \GCN layers, where each \GCN contains 64 hidden units.
%	To avoid over-fitting, we apply L2 regularization with $\lambda=0.00001$ and utilize dropout with $\mathrm{dropout rate}=0.1$.
%	These hyperparameters were found to give the best overall performance in our experiments.

    \cparagraph{Implementation variants}
    	We evaluate our approach on three variant implementations: (1) a two-layered \HRGCN (SE+AE) which combines semantic embedding and
attribute embedding, (2) a two-layered \HRGCN (SE) which only performs semantic embedding with pre-trained word vectors, (3) two-layered
\HRGCN (w/o X) which just initializes each node representation with a unique one-hot vector without considering the semantic or attribute
information mentioned in Section~\ref{subsection:Node Representations}.

    \cparagraph{Prior methods}
    On the $DBP15K_{FR-EN}$ dataset, we compare our \HRGCN model against three methods that have been tuned and tested on the dataset:
    JE~\cite{hao2016joint}, MTransE~\cite{chen2016multilingual} and JAPE~\cite{sun2017cross}. On our WBD dataset, we compare our \HRGCN
    against two representative approaches: JE and ITransE~\cite{zhu2017iterative}. We note that MTransE needs aligned triples for training,
    while JAPE requires aligned relations and attributes (see Table~\ref{seed}). Since our WBD dataset does not have such
    seed alignment data, we do not apply MTransE and JAPE on the WBD datasets. We extend ITransE to allow it work on heterogeneous
    \KGs and refer this extension as ITransE$'$.
%
%ITransE requires all relations to be shared between two \KGs, which is a too strong assumption for heterogeneous \KGs. To provide a fair
%comparison, we build ITransE$'$, a variant of ITransE which does not require the \KGs to have the same relations. 	


	
%	\cparagraph{Variants of our model} We evaluate our approach on five variant implementations: two-layered \HRGCN (S$\|$A) which directly
%uses concatenated vectors introduced in Section~\ref{combine} as input node features, two-layered \HRGCN (SE+AE) which combines semantic
%embedding and attribute embedding, two-layered \HRGCN (SE) which only performs semantic embedding with pre-trained word vectors,
%two-layered \HRGCN (w/o X) which does not use the predefined feature vectors mentioned in Section~\ref{subsection:Node
%		Representations} as the input, and two-layered no-highway \RGCN (SE+AE).
%	



	\subsection{Evaluation Methodology}
    	For the $DBP15K_{FR-EN}$ dataset, we use the same split of gold standards as JAPE~\cite{sun2017cross}: 30\% for training and 70\% for
    testing.
	For semantic embedding, we first directly utilize Google Translate to translate French entities and relations into English.
	Then, we use pre-trained English word vectors \emph{glove.840B.300d}~\footnote{http://nlp.stanford.edu/projects/glove/} to construct the input node feature vectors.
	For attribute embedding, we select 180 high-frequency attributes from each \KG and generate 180-dimensional attribute vectors.	


	For the WBD dataset, we break the gold standards for two parts with equal-size, and use one for training and the other for testing.
	For semantic embeddings, we apply a word2vec~\cite{Mikolov2013Efficient} model to generate 100-dimensional Chinese word embeddings from training the sentences in Baidu dataset.
	By counting the frequency of attributes, we select 63 high-frequency attributes from each \KG to construct 63-dimensional attribute vectors.
    %We use a window size of 5 and a threshold of 20 for downsampling the frequent words
    %for a word2vec model.


 %	All the neural networks are trained using the Adam optimizer~\cite{Kingma2014Adam} for a maximum of 200 epochs with a
% learning rate of 0.01.  Moreover, all models are initialized using Glorot initialization~\cite{Glorot2010Understanding}.

	
 	We use Hits@k, a widely used metric for entity
 alignment evaluation~\cite{hao2016joint,chen2016multilingual,sun2017cross,zhu2017iterative}. A Hits@k score (higher is better) is computed by
 measuring the proportion of correctly aligned entities ranked in the top k. Like prior work, we report the Hits@1 and Hits@10 scores.
	
